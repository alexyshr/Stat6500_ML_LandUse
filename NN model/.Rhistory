#set the name to data + coarseness value
name[1] <- paste("trainingData", i*20, sep = "")
name[2] <- paste("testingData", i*20, sep = "")
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
#assign the first column and the columns between first and last
#assign(name[[1]], trainData[,c(1,begin:ending)])
#assign(name[[2]], testData[,c(1,begin:ending)])
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
trainingList[[i]] <- setNames(trainingList[[i]], name[1])
testingList[[i]] <- testData[,c(1,begin:ending)]
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
for( i in 1:7){
#set the name to data + coarseness value
name[1] <- paste("trainingData", i*20, sep = "")
name[2] <- paste("testingData", i*20, sep = "")
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
#assign the first column and the columns between first and last
#assign(name[[1]], trainData[,c(1,begin:ending)])
#assign(name[[2]], testData[,c(1,begin:ending)])
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
trainingList[i] <- setNames(trainingList[[i]], name[1])
testingList[[i]] <- testData[,c(1,begin:ending)]
}
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical data
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# 2- Brdindx, Shpindx, compact, and round can be omitted from the model since rect is highly correlated with all of them
# 3- Mean_G, Mean_R, and Mean_NIR can be omitted from the model since bright is highly correlated with all of them
# 4- SD_G, SD_R, or SD_NIR should be used in the model; whichever produces the best results
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list(); name <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#set the name to data + coarseness value
name[1] <- paste("trainingData", i*20, sep = "")
name[2] <- paste("testingData", i*20, sep = "")
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
#assign the first column and the columns between first and last
#assign(name[[1]], trainData[,c(1,begin:ending)])
#assign(name[[2]], testData[,c(1,begin:ending)])
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
trainingList[i] <- setNames(trainingList[[i]], name[1])
testingList[[i]] <- testData[,c(1,begin:ending)]
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical data
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# 2- Brdindx, Shpindx, compact, and round can be omitted from the model since rect is highly correlated with all of them
# 3- Mean_G, Mean_R, and Mean_NIR can be omitted from the model since bright is highly correlated with all of them
# 4- SD_G, SD_R, or SD_NIR should be used in the model; whichever produces the best results
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list(); name <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#set the name to data + coarseness value
name[1] <- paste("trainingData", i*20, sep = "")
name[2] <- paste("testingData", i*20, sep = "")
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
#assign the first column and the columns between first and last
#assign(name[[1]], trainData[,c(1,begin:ending)])
#assign(name[[2]], testData[,c(1,begin:ending)])
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
trainingList[i] <- setNames(trainingList[i], name[1])
testingList[[i]] <- testData[,c(1,begin:ending)]
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
View(trainingList)
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical data
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# 2- Brdindx, Shpindx, compact, and round can be omitted from the model since rect is highly correlated with all of them
# 3- Mean_G, Mean_R, and Mean_NIR can be omitted from the model since bright is highly correlated with all of them
# 4- SD_G, SD_R, or SD_NIR should be used in the model; whichever produces the best results
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list(); name <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#set the name to data + coarseness value
name[1] <- paste("trainingData", i*20, sep = "")
name[2] <- paste("testingData", i*20, sep = "")
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
#assign the first column and the columns between first and last
#assign(name[[1]], trainData[,c(1,begin:ending)])
#assign(name[[2]], testData[,c(1,begin:ending)])
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
names(trainingList)[i] <-  name[1]
testingList[[i]] <- testData[,c(1,begin:ending)]
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
View(trainingList)
## 1- load the data files
trainData <- read.csv("training.csv")
View(trainData)
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
trainData$BrdIndx <- NULL
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
trainData[6,]<- NULL
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
trainData[6]<- NULL
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
red <- c(2,3)
trainData[red]<- NULL
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical data
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
red <- c(2,3)
trainData[red]<- NULL
View(testData)
View(testData)
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
# create a list of variables to be deleted
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
partialList <- c(2,4,6,7,8,9,10,12,13) #deletions will be in the loop
partialList <- c(2,4,6,7,8,9,10,12,13)
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical)
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# create a list of variables to be deleted
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
partialList <- c(2,4,6,7,8,9,10,12,13) #deletions will be in the loop
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list()
parTrainingList<- list(); parTestingList <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
#name the coarseness level
names(trainingList)[i] <-  paste("trainingData", i*20, sep = "")
testingList[[i]] <- testData[,c(1,begin:ending)]
#name the coarseness level
names(testingList)[i] <-  paste("testingData", i*20, sep = "")
#trainData[partialList]<- NULL
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
tt <- trainingList[-partialList]
View(tt)
tt <- trainingList[-partialList,]
tt <- trainingList[,-partialList]
tt <- trainingList[[1]][,-partialList]
View(tt)
View(trainingList)
View(trainingList[["trainingData20"]])
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical)
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# create a list of variables to be deleted
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
partialList <- c(2,4,6,7,8,9,10,12,13) #deletions will be in the loop
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list()
parTrainingList<- list(); parTestingList <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
#name the coarseness level
names(trainingList)[i] <-  paste("trainingData", i*20, sep = "")
testingList[[i]] <- testData[,c(1,begin:ending)]
#name the coarseness level
names(testingList)[i] <-  paste("testingData", i*20, sep = "")
#trainData[partialList]<- NULL
parTrainingList[[i]] <- trainingList[[i]][,-partialList]
parTestingList[[i]] <- testingList[[i]][,-partialList]
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
View(parTestingList)
View(testingList)
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical)
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# create a list of variables to be deleted
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
partialList <- c(2,4,6,7,8,9,10,12,13) #deletions will be in the loop
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list()
parTrainingList<- list(); parTestingList <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
#name the coarseness level
names(trainingList)[i] <-  paste("trainingData", i*20, sep = "")
testingList[[i]] <- testData[,c(1,begin:ending)]
#name the coarseness level
names(testingList)[i] <-  paste("testingData", i*20, sep = "")
#Create and name the partial datasets
parTrainingList[[i]] <- trainingList[[i]][,-partialList]
names(parTrainingList)[i] <-  paste("parTrainingData", i*20, sep = "")
parTestingList[[i]] <- testingList[[i]][,-partialList]
names(parTestingList)[i] <-  paste("parTestingData", i*20, sep = "")
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData)
# Now all variables in the work space will called into "NN_Processor.Rmd"
View(parTrainingList)
View(parTestingList)
View(testingList)
# preprocessor fetches the data and separate the different coarseness levels
## 1- load the data files
trainData <- read.csv("training.csv")
testData <- read.csv("testing.csv")
## 2- check for missing or NaN values
#check for missing data in both data sets
sum(is.na(trainData)); sum(is.na(testData))
# check for NaN/infinite data, since all attributes are numerical, we check with is.infinite()
sum(!sapply(trainData[,!1], is.finite)); sum(!sapply(testData[,!1], is.finite))
#no missing or NaN data
## 3- Change to categorical)
# convert class from "char" into a factor
trainData$class <- as.factor(trainData$class)
testData$class <- as.factor(testData$class)
# create a list of variables to be deleted
# omitted variables: Brdindx, Shpindx, compact, round, Mean_G, Mean_R, Mean_NIR, SD_R, SD_NIR
partialList <- c(2,4,6,7,8,9,10,12,13) #deletions will be in the loop
## 4- separate the data into 7 coarseness levels (2 sets: full and partial (EDA result))
#create a list of dataframes
trainingList <- list(); testingList <- list()
parTrainingList<- list(); parTestingList <- list()
#create new dataframe for each coarseness level
for( i in 1:7){
#first column index of the selected coarseness
begin = (i-1)*21+2
#last column index
ending = begin+20
# add to the list
trainingList[[i]] <- trainData[,c(1,begin:ending)]
#name the coarseness level
names(trainingList)[i] <-  paste("trainingData", i*20, sep = "")
testingList[[i]] <- testData[,c(1,begin:ending)]
#name the coarseness level
names(testingList)[i] <-  paste("testingData", i*20, sep = "")
#Create and name the partial datasets
parTrainingList[[i]] <- trainingList[[i]][,-partialList]
names(parTrainingList)[i] <-  paste("parTrainingData", i*20, sep = "")
parTestingList[[i]] <- testingList[[i]][,-partialList]
names(parTestingList)[i] <-  paste("parTestingData", i*20, sep = "")
}
## 5- clean the work space
rm(begin); rm(ending); rm(i); rm(trainData); rm(testData); rm(partialList)
# Now all variables in the work space will called into "NN_Processor.Rmd"
knitr::opts_chunk$set(echo = TRUE)
library(neuralnet)
#This routine only loads the data from original .csv files, and separate them into each coarseness level, no need to write the code here since its function is trivial
source('preprocessor.r', local = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(neuralnet)
help("neuralnet")
parTrainingList[[1]][1]
View(parTrainingList)
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, testingData20)
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTrestingList[[1]])
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[1]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[1]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = TrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = trainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, testingList[[1]])
#confusion matrix, actual vs. predicted
t3 <- table(testingList[[1]]$class, apply(pr, 1, which.max))
t3
knitr::opts_chunk$set(echo = TRUE)
library(neuralnet)
#This routine only loads the data from original .csv files, and separate them into each coarseness level, no need to write the code here since its function is trivial
source('preprocessor.r', local = TRUE)
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = trainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, testingList[[1]])
#confusion matrix, actual vs. predicted
t3 <- table(testingList[[1]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[1]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[1]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = c(10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = 13, algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[1]], hidden = 13, algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = 13, algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = 10, algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[2]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[2]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = c(10,10,10), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[2]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[2]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = c(5,5,5), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[2]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[2]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = c(13,9), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
#predicting values of the testing coarseness level counterpart
pr <- predict(fit, parTestingList[[2]])
#confusion matrix, actual vs. predicted
t3 <- table(parTestingList[[2]]$class, apply(pr, 1, which.max))
t3
# This code is just for initial testing and debugging
# fitting a neural network, using training data, 2 hidden layers, 10 nodes each
fit <- neuralnet(class~. , data = parTrainingList[[2]], hidden = c(13,13), algorithm = "rprop+", linear.output = FALSE)
#plotting the NN
plot(fit) # move to 'postprocessor.R' after debugging
View(fit)
fit[["result.matrix"]]
